# Gesture-Based Human-Computer Interaction System

![badge](https://img.shields.io/badge/Python-3.10+-blue) ![badge](https://img.shields.io/badge/OpenCV-Computer%20Vision-success) ![badge](https://img.shields.io/badge/MediaPipe-Hand%20Tracking-informational) ![badge](https://img.shields.io/badge/NLTK-Sentiment-lightgrey) ![badge](https://img.shields.io/badge/License-MIT-yellow.svg)

Control your computer using hand gestures via webcam. Includes a sentiment-aware chatbot demo.

> **Status:** Public, MIT-licensed, open to issues and PRs.

## âœ¨ Features
- Real-time processing
- Lightweight demo code that runs on most laptops
- Clean project structure
- Ready-to-use `requirements.txt`
- Screenshots and architecture diagram included

## ğŸ§° Tech Stack
- Python, OpenCV, NumPy
- MediaPipe, NLTK, scikit-learn

## ğŸš€ Quickstart
```bash
# 1) Create and activate a virtual environment
python -m venv .venv
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) Run the demo
python src/main.py
```

## ğŸ“¦ Installation (Alt)
- You can skip heavy deps if you only want to see the windowed demo.
- For Twilio SMS, copy `src/config.example.toml` to `config.toml` and fill credentials. (Safeguard only)

## ğŸ–¼ï¸ Screenshots
![Banner](assets/banner.png)
![Architecture](assets/architecture.png)
![Screenshot](assets/screenshot.png)

## ğŸ—‚ï¸ Project Structure
```
.
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ banner.png
â”‚   â”œâ”€â”€ architecture.png
â”‚   â””â”€â”€ screenshot.png
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ¤ Contributing
- Fork the repo and open a PR.
- Good first issues: docs improvements, performance tweaks, and cross-platform fixes.

## ğŸ“ License
This project is released under the MIT License.
